python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 32 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 64 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 32 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 64 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 1 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 2 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 2 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 3 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python main.py --trace alibaba2018 --epochs 1000 --seq_len 288 --batch_size 128 --hidden_dim 128 --dropout 0 --narrow_attn_heads 4 --lr 0.001 --num_layers 4 --encoder_decoder_model Transformer --device cuda --experiment_save_dir ~/experiments/seq-to-seq/transformer/alibaba2018/
python predict.py --experiment_directory_path ~/experiments/seq-to-seq/transformer/alibaba2018/ --n_samples_export 10 --recursive true
